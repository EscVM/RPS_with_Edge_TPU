{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Keras to TensorfloLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the folder path where is located the model \n",
    "DIR = './bin'\n",
    "name_model = 'rps.h5'\n",
    "\n",
    "# input size model\n",
    "input_size = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-29 11:07:19--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.206.80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.206.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 200682221 (191M) [application/zip]\n",
      "Saving to: '/tmp/rps.zip'\n",
      "\n",
      "/tmp/rps.zip        100%[===================>] 191.38M   108MB/s    in 1.8s    \n",
      "\n",
      "2020-01-29 11:07:21 (108 MB/s) - '/tmp/rps.zip' saved [200682221/200682221]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the training dataset in /tmp if you don't have it in your path\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
    "    -O /tmp/rps.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the training \n",
    "local_zip = '/tmp/rps.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rock images: 840\n",
      "total training paper images: 840\n",
      "total training scissors images: 840\n"
     ]
    }
   ],
   "source": [
    "rock_dir = os.path.join('/tmp/rps/rock')\n",
    "paper_dir = os.path.join('/tmp/rps/paper')\n",
    "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
    "rock_files = os.listdir(rock_dir)\n",
    "paper_files = os.listdir(paper_dir)\n",
    "scissors_files = os.listdir(scissors_dir)\n",
    "\n",
    "print('total training rock images:', len(os.listdir(rock_dir)))\n",
    "print('total training paper images:', len(os.listdir(paper_dir)))\n",
    "print('total training scissors images:', len(os.listdir(scissors_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select n_images from the three classes\n",
    "n_images = 600\n",
    "\n",
    "next_rock = [os.path.join(rock_dir, fname) \n",
    "                for fname in rock_files[:n_images]]\n",
    "next_paper = [os.path.join(paper_dir, fname) \n",
    "                for fname in paper_files[:n_images]]\n",
    "next_scissors = [os.path.join(scissors_dir, fname) \n",
    "                for fname in scissors_files[:n_images]]\n",
    "\n",
    "imgs = next_rock + next_paper + next_scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the selected images with matplotlib :)\n",
    "dataset = np.empty((n_images*3, input_size, input_size, 3))\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    img = mpimg.imread(imgs[i]) \n",
    "    img = cv2.resize(img, (input_size, input_size))\n",
    "    dataset[i,:,:,:] = img[:,:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Convert to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset generator\n",
    "def representative_dataset_gen():\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        imgs = dataset[i:i+1]\n",
    "        yield [imgs.astype('float32')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the converter loading the model\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(os.path.join(DIR, name_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the converter for a full integer quantization of weights and activations\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TensorFlowLite\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the \n",
    "tflite_model_file = pathlib.Path('./bin/rps.tflite')\n",
    "tflite_model_file.write_bytes(tflite_quant_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
